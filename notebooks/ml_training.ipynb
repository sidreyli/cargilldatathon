{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Port Congestion Prediction Model Training\n",
    "\n",
    "This notebook trains a LightGBM model to predict port waiting times at bulk carrier discharge ports.\n",
    "\n",
    "**Target Ports:**\n",
    "- China: Qingdao, Rizhao, Caofeidian, Fangcheng\n",
    "- India: Mundra, Vizag\n",
    "\n",
    "**Target Metrics:**\n",
    "- MAE < 1.5 days\n",
    "- RMSE < 2.0 days\n",
    "- % within 1 day of actual > 60%\n",
    "- % within 2 days of actual > 80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML imports\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import joblib\n",
    "\n",
    "# Local imports\n",
    "from ml_models.feature_engineering import FeatureEngineer\n",
    "from ml_models.holiday_calendar import HolidayCalendar\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading\n",
    "\n",
    "Load the 5M row daily port activity data and filter to target ports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "print(\"Loading port activity data...\")\n",
    "activity_df = pd.read_csv('Daily_Port_Activity_Data_and_Trade_Estimates.csv')\n",
    "print(f\"Loaded {len(activity_df):,} rows\")\n",
    "\n",
    "# Load port database for capacity info\n",
    "ports_df = pd.read_csv('PortWatch_ports_database.csv')\n",
    "print(f\"Loaded {len(ports_df):,} ports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target port IDs\n",
    "TARGET_PORTS = {\n",
    "    'port1069': 'Qingdao',\n",
    "    'port1105': 'Rizhao',\n",
    "    'port339': 'Fangcheng',\n",
    "    'port1266': 'Caofeidian',\n",
    "    'port777': 'Mundra',\n",
    "    'port1367': 'Vizag',\n",
    "}\n",
    "\n",
    "# Filter to target ports\n",
    "target_df = activity_df[activity_df['portid'].isin(TARGET_PORTS.keys())].copy()\n",
    "target_df['date'] = pd.to_datetime(target_df['date'])\n",
    "target_df = target_df.sort_values(['portid', 'date'])\n",
    "\n",
    "print(f\"Filtered to {len(target_df):,} rows for target ports\")\n",
    "print(f\"Date range: {target_df['date'].min()} to {target_df['date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show data per port\n",
    "port_counts = target_df.groupby('portid').size().reset_index(name='count')\n",
    "port_counts['port_name'] = port_counts['portid'].map(TARGET_PORTS)\n",
    "print(\"\\nRows per port:\")\n",
    "print(port_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot port calls distribution\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (port_id, port_name) in enumerate(TARGET_PORTS.items()):\n",
    "    port_data = target_df[target_df['portid'] == port_id]\n",
    "    axes[idx].hist(port_data['portcalls_dry_bulk'], bins=50, alpha=0.7)\n",
    "    axes[idx].set_title(f'{port_name} - Port Calls Distribution')\n",
    "    axes[idx].set_xlabel('Daily Port Calls')\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series plot\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (port_id, port_name) in enumerate(TARGET_PORTS.items()):\n",
    "    port_data = target_df[target_df['portid'] == port_id].set_index('date')\n",
    "    # Rolling 7-day average\n",
    "    rolling = port_data['portcalls_dry_bulk'].rolling(7).mean()\n",
    "    axes[idx].plot(rolling, alpha=0.8)\n",
    "    axes[idx].set_title(f'{port_name} - 7-Day Rolling Port Calls')\n",
    "    axes[idx].set_xlabel('Date')\n",
    "    axes[idx].set_ylabel('Port Calls (7-day avg)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seasonality analysis - Monthly patterns\n",
    "target_df['month'] = target_df['date'].dt.month\n",
    "\n",
    "monthly_avg = target_df.groupby(['portid', 'month'])['portcalls_dry_bulk'].mean().reset_index()\n",
    "monthly_avg['port_name'] = monthly_avg['portid'].map(TARGET_PORTS)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "for port_id, port_name in TARGET_PORTS.items():\n",
    "    port_monthly = monthly_avg[monthly_avg['portid'] == port_id]\n",
    "    ax.plot(port_monthly['month'], port_monthly['portcalls_dry_bulk'], marker='o', label=port_name)\n",
    "\n",
    "ax.set_xlabel('Month')\n",
    "ax.set_ylabel('Average Daily Port Calls')\n",
    "ax.set_title('Seasonal Patterns by Port')\n",
    "ax.legend()\n",
    "ax.set_xticks(range(1, 13))\n",
    "ax.set_xticklabels(['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Target Variable Creation\n",
    "\n",
    "Create the congestion proxy (delay_days) target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize feature engineer\n",
    "feature_engineer = FeatureEngineer('PortWatch_ports_database.csv')\n",
    "\n",
    "# Create target variable for each port\n",
    "all_data = []\n",
    "\n",
    "for port_id in TARGET_PORTS.keys():\n",
    "    port_data = target_df[target_df['portid'] == port_id].copy()\n",
    "    port_data = port_data.sort_values('date')\n",
    "    \n",
    "    # Create delay_days target\n",
    "    port_data['delay_days'] = feature_engineer.create_target_variable(port_data, port_id)\n",
    "    all_data.append(port_data)\n",
    "    \n",
    "    print(f\"{TARGET_PORTS[port_id]}: mean delay = {port_data['delay_days'].mean():.2f} days\")\n",
    "\n",
    "combined_df = pd.concat(all_data, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot target variable distribution\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (port_id, port_name) in enumerate(TARGET_PORTS.items()):\n",
    "    port_data = combined_df[combined_df['portid'] == port_id]\n",
    "    axes[idx].hist(port_data['delay_days'], bins=30, alpha=0.7, color='coral')\n",
    "    axes[idx].axvline(port_data['delay_days'].mean(), color='red', linestyle='--', label='Mean')\n",
    "    axes[idx].set_title(f'{port_name} - Delay Distribution')\n",
    "    axes[idx].set_xlabel('Delay (days)')\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "    axes[idx].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Engineer features for all ports\n",
    "all_features = []\n",
    "\n",
    "for port_id in TARGET_PORTS.keys():\n",
    "    port_data = combined_df[combined_df['portid'] == port_id].copy()\n",
    "    port_data = port_data.sort_values('date')\n",
    "    \n",
    "    # Full feature engineering\n",
    "    features_df = feature_engineer.engineer_features(port_data, port_id, include_target=True)\n",
    "    all_features.append(features_df)\n",
    "    \n",
    "    print(f\"{TARGET_PORTS[port_id]}: {len(features_df)} rows, {len(features_df.columns)} features\")\n",
    "\n",
    "training_df = pd.concat(all_features, ignore_index=True)\n",
    "print(f\"\\nTotal training data: {len(training_df):,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show feature columns\n",
    "print(\"Feature columns:\")\n",
    "print(training_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with NaN from lag features\n",
    "feature_cols = feature_engineer.get_feature_columns()\n",
    "available_features = [c for c in feature_cols if c in training_df.columns]\n",
    "\n",
    "print(f\"Using {len(available_features)} features\")\n",
    "print(available_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare train/validation/test splits (time-based)\n",
    "training_df['date'] = pd.to_datetime(training_df['date'])\n",
    "\n",
    "# Train: 2019-01-01 to 2023-12-31\n",
    "# Validation: 2024-01-01 to 2024-06-30\n",
    "# Test: 2024-07-01 to latest\n",
    "\n",
    "train_mask = training_df['date'] < '2024-01-01'\n",
    "val_mask = (training_df['date'] >= '2024-01-01') & (training_df['date'] < '2024-07-01')\n",
    "test_mask = training_df['date'] >= '2024-07-01'\n",
    "\n",
    "train_df = training_df[train_mask].dropna(subset=['delay_days'])\n",
    "val_df = training_df[val_mask].dropna(subset=['delay_days'])\n",
    "test_df = training_df[test_mask].dropna(subset=['delay_days'])\n",
    "\n",
    "print(f\"Train: {len(train_df):,} rows ({train_df['date'].min()} to {train_df['date'].max()})\")\n",
    "print(f\"Val:   {len(val_df):,} rows ({val_df['date'].min()} to {val_df['date'].max()})\")\n",
    "print(f\"Test:  {len(test_df):,} rows ({test_df['date'].min()} to {test_df['date'].max()})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare feature matrices\n",
    "X_train = train_df[available_features].fillna(0)\n",
    "y_train = train_df['delay_days']\n",
    "\n",
    "X_val = val_df[available_features].fillna(0)\n",
    "y_val = val_df['delay_days']\n",
    "\n",
    "X_test = test_df[available_features].fillna(0)\n",
    "y_test = test_df['delay_days']\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM parameters\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': ['mae', 'rmse'],\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'min_data_in_leaf': 100,\n",
    "    'verbose': -1,\n",
    "    'seed': 42,\n",
    "}\n",
    "\n",
    "# Create datasets\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "\n",
    "# Train model\n",
    "print(\"Training LightGBM model...\")\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    num_boost_round=1000,\n",
    "    valid_sets=[train_data, val_data],\n",
    "    valid_names=['train', 'val'],\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=50),\n",
    "        lgb.log_evaluation(period=100)\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"\\nBest iteration: {model.best_iteration}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_val_pred = model.predict(X_val)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "def calculate_metrics(y_true, y_pred, name):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    within_1_day = np.mean(np.abs(y_true - y_pred) <= 1) * 100\n",
    "    within_2_days = np.mean(np.abs(y_true - y_pred) <= 2) * 100\n",
    "    \n",
    "    print(f\"\\n{name} Metrics:\")\n",
    "    print(f\"  MAE: {mae:.3f} days\")\n",
    "    print(f\"  RMSE: {rmse:.3f} days\")\n",
    "    print(f\"  Within 1 day: {within_1_day:.1f}%\")\n",
    "    print(f\"  Within 2 days: {within_2_days:.1f}%\")\n",
    "    \n",
    "    return {'mae': mae, 'rmse': rmse, 'within_1_day': within_1_day, 'within_2_days': within_2_days}\n",
    "\n",
    "train_metrics = calculate_metrics(y_train, y_train_pred, \"Training\")\n",
    "val_metrics = calculate_metrics(y_val, y_val_pred, \"Validation\")\n",
    "test_metrics = calculate_metrics(y_test, y_test_pred, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we meet target metrics\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TARGET METRICS CHECK (Test Set):\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "checks = [\n",
    "    (\"MAE < 1.5 days\", test_metrics['mae'] < 1.5),\n",
    "    (\"RMSE < 2.0 days\", test_metrics['rmse'] < 2.0),\n",
    "    (\"Within 1 day > 60%\", test_metrics['within_1_day'] > 60),\n",
    "    (\"Within 2 days > 80%\", test_metrics['within_2_days'] > 80),\n",
    "]\n",
    "\n",
    "for check_name, passed in checks:\n",
    "    status = \"PASS\" if passed else \"FAIL\"\n",
    "    print(f\"  [{status}] {check_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-port metrics\n",
    "print(\"\\nPer-Port Test Metrics:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for port_id, port_name in TARGET_PORTS.items():\n",
    "    port_mask = test_df['portid'] == port_id\n",
    "    if port_mask.sum() > 0:\n",
    "        port_y_test = y_test[port_mask]\n",
    "        port_y_pred = y_test_pred[port_mask]\n",
    "        \n",
    "        mae = mean_absolute_error(port_y_test, port_y_pred)\n",
    "        print(f\"  {port_name}: MAE = {mae:.2f} days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual vs predicted\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for ax, (y_true, y_pred, name) in zip(axes, \n",
    "    [(y_train, y_train_pred, 'Train'), (y_val, y_val_pred, 'Validation'), (y_test, y_test_pred, 'Test')]):\n",
    "    ax.scatter(y_true, y_pred, alpha=0.3, s=5)\n",
    "    ax.plot([0, 15], [0, 15], 'r--', label='Perfect')\n",
    "    ax.set_xlabel('Actual Delay (days)')\n",
    "    ax.set_ylabel('Predicted Delay (days)')\n",
    "    ax.set_title(f'{name} Set')\n",
    "    ax.set_xlim(0, 15)\n",
    "    ax.set_ylim(0, 15)\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': available_features,\n",
    "    'importance': model.feature_importance(importance_type='gain')\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 15 Features by Importance:\")\n",
    "print(importance_df.head(15).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(importance_df['feature'].head(15), importance_df['importance'].head(15))\n",
    "plt.xlabel('Feature Importance (Gain)')\n",
    "plt.title('Top 15 Most Important Features')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP analysis (optional - requires shap package)\n",
    "try:\n",
    "    import shap\n",
    "    \n",
    "    # Sample for SHAP (too slow on full dataset)\n",
    "    sample_size = min(1000, len(X_test))\n",
    "    X_sample = X_test.sample(sample_size, random_state=42)\n",
    "    \n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X_sample)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    shap.summary_plot(shap_values, X_sample, show=False)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"SHAP not installed. Install with: pip install shap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "import os\n",
    "os.makedirs('ml_models/saved_models', exist_ok=True)\n",
    "\n",
    "model_path = 'ml_models/saved_models/port_delay_v1.joblib'\n",
    "joblib.dump(model, model_path)\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "\n",
    "# Verify loading\n",
    "loaded_model = joblib.load(model_path)\n",
    "test_pred = loaded_model.predict(X_test[:5])\n",
    "print(f\"\\nTest predictions after reload: {test_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save feature list for reference\n",
    "import json\n",
    "\n",
    "model_info = {\n",
    "    'model_version': 'v1',\n",
    "    'training_date': datetime.now().isoformat(),\n",
    "    'features': available_features,\n",
    "    'target_ports': TARGET_PORTS,\n",
    "    'test_metrics': {\n",
    "        'mae': float(test_metrics['mae']),\n",
    "        'rmse': float(test_metrics['rmse']),\n",
    "        'within_1_day_pct': float(test_metrics['within_1_day']),\n",
    "        'within_2_days_pct': float(test_metrics['within_2_days']),\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('ml_models/saved_models/model_info.json', 'w') as f:\n",
    "    json.dump(model_info, f, indent=2)\n",
    "\n",
    "print(\"Model info saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test Integration with FreightCalculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the predictor class\n",
    "from ml_models.port_congestion_predictor import PortCongestionPredictor\n",
    "\n",
    "predictor = PortCongestionPredictor(\n",
    "    model_path='ml_models/saved_models/port_delay_v1.joblib',\n",
    "    data_path='Daily_Port_Activity_Data_and_Trade_Estimates.csv',\n",
    "    port_database_path='PortWatch_ports_database.csv'\n",
    ")\n",
    "\n",
    "print(f\"Model available: {predictor.is_model_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test predictions\n",
    "test_ports = ['Qingdao', 'Rizhao', 'Mundra', 'Vizag']\n",
    "test_date = '2026-03-15'\n",
    "\n",
    "print(f\"Predictions for {test_date}:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for port in test_ports:\n",
    "    result = predictor.predict(port, test_date)\n",
    "    print(f\"  {port}: {result.predicted_delay_days:.1f} days \"\n",
    "          f\"[{result.confidence_lower:.1f} - {result.confidence_upper:.1f}] \"\n",
    "          f\"({result.congestion_level}) \"\n",
    "          f\"[{result.model_used}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test voyage delay function\n",
    "delay = predictor.get_delay_for_voyage(\n",
    "    discharge_port='Qingdao',\n",
    "    eta_date='2026-03-15',\n",
    "    cargo_quantity=170000\n",
    ")\n",
    "\n",
    "print(f\"\\nVoyage delay for Qingdao on 2026-03-15: {delay:.1f} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The port congestion prediction model has been trained and saved. Key outputs:\n",
    "\n",
    "1. **Model**: `ml_models/saved_models/port_delay_v1.joblib`\n",
    "2. **Model Info**: `ml_models/saved_models/model_info.json`\n",
    "\n",
    "**Usage in FreightCalculator**:\n",
    "```python\n",
    "from ml_models import PortCongestionPredictor\n",
    "\n",
    "predictor = PortCongestionPredictor('ml_models/saved_models/port_delay_v1.joblib')\n",
    "delay = predictor.get_delay_for_voyage(\"Qingdao\", \"2026-03-15\")\n",
    "result = calculator.calculate_voyage(vessel, cargo, extra_port_delay_days=delay)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
